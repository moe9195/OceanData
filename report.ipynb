{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CNN to predict vorticity from temperature and kinetic energy data\n",
    "\n",
    "I've implemented a convolutional neural network with five hidden layers which takes an input 75x75 grid of temperature or kinetic energy readings and predicts the vorticity in those regions. \n",
    "\n",
    "The dataset used to train the network consists of CMEMS data from 1993 to 2016 of various oceans. Each 600x600 grid of data was split into 64 75x75 grids where each pixel represents 1/12 degrees as shown in the image below.\n",
    "\n",
    "<img src=\"full_grid_KE.png\" alt=\"strain&deformationtype\" width=\"300\"/>\n",
    "\n",
    "Doing this, I was able to create a dataset with 142,000 input output image pairs which I used to train the network. Below, I show my code as well as the results of the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, Reshape\n",
    "from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I wrote a library of functions which will be useful in the future "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a vector of x-locations and a vector of y-locations \n",
    "# and calculates the slope (dy/dx) at each x-location.\n",
    "def slope(y, x):\n",
    "    l = np.size(y)\n",
    "    s = np.zeros(l)\n",
    "    s[0] = (y[1] - y[0]) / (x[1] - x[0])\n",
    "    s[l - 1] = (y[l - 1] - y[l - 2]) / (x[l - 1] - x[l - 2])\n",
    "    for i in range(1, l - 1, 1):\n",
    "        s[i] = (y[i + 1] - y[i - 1]) / (x[i + 1] - x[i - 1])\n",
    "    return s\n",
    "\n",
    "# calculates curl of a vector F on a 2D rectangular grid\n",
    "def curl(x, y, Fx, Fy):\n",
    "    dFy_dx = np.zeros((len(y), len(x)))\n",
    "    dFx_dy = np.zeros((len(y), len(x)))\n",
    "\n",
    "    for iy in range(len(y)):\n",
    "        dFy_dx[iy, :] = slope(np.ravel(Fy[iy, :]), x)\n",
    "\n",
    "    for ix in range(len(x)):\n",
    "        dFx_dy[:, ix] = slope(np.ravel(Fx[:, ix]), y)\n",
    "\n",
    "    return dFy_dx - dFx_dy\n",
    "\n",
    "# calculates the vorticity when given the ocean data from the get_data function\n",
    "def vorticity(data):\n",
    "    lons, lats, vx, vy = data[0], data[1], data[2], data[3]\n",
    "    w, h, t_total = np.shape(vx)[1], np.shape(vx)[2], np.shape(vx)[0]\n",
    "    vor = np.zeros((t_total, w, h), dtype=np.float32)\n",
    "    for i in range(t_total):\n",
    "        vor[i, :, :] = curl(lons, lats, vx[i, :], vy[i, :])\n",
    "    return vor\n",
    "\n",
    "# takes the .nc filename and outputs its data as numpy arrays\n",
    "def get_data(filename, n):\n",
    "    ds = xr.open_dataset(filename)\n",
    "\n",
    "    d, time = ds.depth, ds.time\n",
    "    lat, long = ds.latitude, ds.longitude\n",
    "\n",
    "    if n == 1:\n",
    "        v_x, v_y = ds.uo, ds.vo\n",
    "        temp = ds.thetao\n",
    "    else:\n",
    "        v_x, v_y = ds.u, ds.v\n",
    "        temp = ds.temperature\n",
    "\n",
    "    temp = temp.values\n",
    "    lats, lons = lat.values, long.values\n",
    "    vx, vy = v_x.values, v_y.values\n",
    "\n",
    "    w = np.shape(vx[0, :])[1]\n",
    "    h = np.shape(vx[0, :])[2]\n",
    "    t_total = len(time.values)\n",
    "    w, h = np.shape(vx[0, :])[1], np.shape(vx[0, :])[2]\n",
    "    vx, vy, temp = vx.reshape((t_total, w, h)), vy.reshape((t_total, w, h)), temp.reshape((t_total, w, h))\n",
    "    data = [lons, lats, vx, vy, temp, time]\n",
    "    return data\n",
    "\n",
    "# takes a 3d matrix and splits each submatrix into 4 submatricies for each iteration\n",
    "# input is an NxMxM matrix and output is N'xM'xM' matrix \n",
    "# where N' = (4^iterations)*N and M' = M/(2^iterations)\n",
    "def split_data(A, iterations):\n",
    "    def split_in_four(A):\n",
    "        shape = np.shape(A)\n",
    "        d, l, w = shape[0], shape[1], shape[2]\n",
    "        div = int(l/2)\n",
    "        upperLeft  = A[0:d,0:div,0:div]\n",
    "        upperRight = A[0:d,0:div,div:2*div]\n",
    "        lowerLeft  = A[0:d,div:2*div,0:div]\n",
    "        lowerRight = A[0:d,div:2*div,div:2*div]\n",
    "        combined   = np.concatenate((upperLeft, upperRight, lowerLeft, lowerRight))\n",
    "        return combined\n",
    "    for i in range(iterations):\n",
    "        A = split_in_four(A)\n",
    "        if np.shape(A)[2] < 2:\n",
    "            break\n",
    "    return A\n",
    "\n",
    "# fills NaN values with mean of each column\n",
    "def fill_nans_simple(A):\n",
    "    A0 = np.empty(np.shape(A))\n",
    "    for i in range(len(A)):\n",
    "        imp = SimpleImputer()\n",
    "        imp.fit(A[i, :])\n",
    "        A0[i, :] = imp.transform(A[i, :])\n",
    "    return A0\n",
    "\n",
    "# fills NaN values by modelling each feature as a function of missing futures\n",
    "# gives better results than simple imputer but much more computationally heavy\n",
    "def fill_nans_iterative(A):\n",
    "    A0 = np.empty(np.shape(A))\n",
    "    for i in range(len(A)):\n",
    "        imp = IterativeImputer()\n",
    "        imp.fit(A[i, :])\n",
    "        A0[i, :] = imp.transform(A[i, :])\n",
    "    return A0\n",
    "\n",
    "# takes an n-dimensional array and normalizes its values to [0, 1]\n",
    "def normalize(x):\n",
    "    return (x - np.min(x))/(np.max(x) - np.min(x))\n",
    "\n",
    "# function for plotting the data. set plot_quiver = 1 to plot vector field,\n",
    "# set plot_vorticity, plot_temperature, or plot_KE to 1 and the others to 0 to\n",
    "# plot the desired variable\n",
    "def plot_data(t, data, plot_quiver,\n",
    "              plot_vorticity, plot_temperature, plot_KE):\n",
    "    \n",
    "    skip = 5 # skips data points when plotting vector field for better visualisation\n",
    "    lons, lats, vx, vy, time, T = data[0], data[1], data[2], data[3], data[4], data[5]\n",
    "    \n",
    "    w, h = np.shape(vx)[1], np.shape(vx)[2]\n",
    "    vx_s, vy_s = vx[t,:].reshape((w,h)), vy[t,:].reshape((w,h))\n",
    "    time = time[t]\n",
    "    \n",
    "    if plot_vorticity:\n",
    "        if not plot_temperature:\n",
    "            vor = curl(lons, lats, vx_s, vy_s)   # calculating vorticity\n",
    "           # vmin = np.nanmin(vor)/2\n",
    "           # vmax = np.nanmax(vor)/2\n",
    "            vmin = -5.5\n",
    "            vmax = 5.5\n",
    "            label = 'Vorticity'\n",
    "        else:\n",
    "            vor = T[t,:]\n",
    "            vmin = np.nanmin(vor)\n",
    "            vmax = np.nanmax(vor)\n",
    "            label = 'Temperature'\n",
    "    if plot_KE:\n",
    "        vor = np.sqrt(vx_s**2 + vy_s**2)/2\n",
    "        vmin = np.nanmin(vor)\n",
    "        vmax = np.nanmax(vor)\n",
    "        label = 'Kinetic Energy'\n",
    "    \n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree(\n",
    "        central_longitude=0.0, globe=None)})\n",
    "    fig.set_size_inches([30,15])\n",
    "\n",
    "    ax.set_global()\n",
    "    ax.stock_img()\n",
    "    ax.text(0.55, 0.95, str(time)[0:10], \n",
    "        transform=ax.transAxes, ha=\"right\", color='black', \n",
    "        bbox=dict(facecolor='white', edgecolor='black', boxstyle='round'))\n",
    "    \n",
    "    if plot_vorticity:   \n",
    "        plt.contourf(lons, lats, vor, 99, levels=np.linspace(-5.5, 5.5, 100, endpoint=True),\n",
    "                     vmin = vmin, vmax = vmax, transform=ccrs.PlateCarree())\n",
    "        \n",
    "        cbaxes = inset_axes(ax, width=\"3.5%\", height=\"45%\", loc=2) \n",
    "        cbar = plt.colorbar(cax=cbaxes, ticks=np.arange(-5,6,1));\n",
    "        cbar.ax.set_ylabel(label, rotation=270, labelpad=10)\n",
    "        \n",
    "    if plot_quiver:\n",
    "        ax.quiver(lons[::skip], lats[::skip], vx_s[::skip, ::skip], vy_s[::skip, ::skip],\n",
    "            transform=ccrs.PlateCarree(), width=0.002)\n",
    "    \n",
    "    ax.set_extent([lons.min(), lons.max(), lats.min(), lats.max()], crs=ccrs.PlateCarree())\n",
    "    plt.savefig('Images/' + 'img-'+str(t)+'.jpeg', bbox_inches = 'tight', pad_inches = 0, dpi=100)\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I downloaded the data from CMEMs in increments (due to the 1 GB download limit). I then wrote a script that reads the .nc files and converts them into numpy arrays, splits them into smaller grids, normalizes their values, and fills all the NaN values then appends and saves them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['nc_data/1.nc', 'nc_data/2.nc', 'nc_data/4.nc', \n",
    "             'nc_data/5.nc', 'nc_data/6.nc', 'nc_data/7.nc']\n",
    "file_len = len(filenames)\n",
    "split = 3\n",
    "x, y, z = int((4**split)*312), int(601/(2**split)), int(601/(2**split))\n",
    "vor_total, KE_total, T_total = (np.empty((file_len*x, y, z), dtype=np.float32)\n",
    "                                , np.empty((file_len*x, y, z), dtype=np.float32)\n",
    "                                , np.empty((file_len*x, y, z), dtype=np.float32))\n",
    "for i in range(file_len):\n",
    "    filename = filenames[i]\n",
    "    data = get_data(filename, 1)\n",
    "    lons, lats, vx, vy, T, time = data\n",
    "    KE = (vx**2 + vy**2)\n",
    "    vx, vy, lons, lats, time = None, None, None, None, None\n",
    "    vor = vorticity(data)\n",
    "    data = None\n",
    "    vor_split = split_data(vor, split)\n",
    "    vor = None\n",
    "    KE_split = split_data(KE, split)\n",
    "    KE = None\n",
    "    T_split = split_data(T, split)\n",
    "    T = None\n",
    "    vor_total[i*x:(i+1)*x,0:y, 0:z] = vor_split\n",
    "    vor_split = None\n",
    "    KE_total[i*x:(i+1)*x,0:y, 0:z] = KE_split\n",
    "    KE_split = None\n",
    "    T_total[i*x:(i+1)*x,0:y, 0:z] = T_split\n",
    "    print(np.shape(T_total))\n",
    "    print(np.shape(T_split))\n",
    "    print(i)\n",
    "    T_split = None\n",
    "\n",
    "vor_total = normalize(fill_nans_simple(vor_total))\n",
    "T_total = normalize(fill_nans_simple(T_total))\n",
    "KE_total = normalize(fill_nans_simple(KE_total))\n",
    "np.save('vorticity', vor_total)\n",
    "np.save('temperature', T_total)\n",
    "np.save('kinetic energy', KE_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I write and train my neural network then save the trained model. The data is split 80:10:10 where 80% of the 140,000 images are used for training and 10% are used for validation and 10% are used for testing. This is done for both temperature -> vorticity and KE -> vorticity. Each was trained for 100 epochs and took approximately 12 hours to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ke = np.load('npy_data_files/kinetic energy.npy')  # can also use temperature\n",
    "vor = np.load('npy_data_files/vorticity.npy')\n",
    "\n",
    "imsize = np.shape(ke)[1]\n",
    "\n",
    "# splitting data into train/validation sets\n",
    "train_X, validation_X, train_Y, validation_Y = model_selection.train_test_split(ke, vor, test_size=0.20)\n",
    "x1, y1, z1 = np.shape(train_X)\n",
    "x2, y2, z2 = np.shape(validation_X)\n",
    "train_X, train_Y = train_X.reshape((x1, y1, z1, 1)), train_Y.reshape((x1, y1, z1, 1))\n",
    "validation_X, validation_Y = validation_X.reshape((x2, y2, z2, 1)), validation_Y.reshape((x2, y2, z2, 1))\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "print('x_train shape:', train_X.shape)\n",
    "print(train_X.shape[0], 'train samples')\n",
    "print(validation_X.shape[0], 'validation samples')\n",
    "\n",
    "# building the network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), input_shape=(imsize, imsize, 1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', dilation_rate=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', dilation_rate=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', dilation_rate=4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3),  padding='same', dilation_rate=4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=keras.losses.mean_squared_error,  \n",
    "              optimizer=keras.optimizers.Adam(), verbose=1,  metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# training the network and saving the trained network and its training history\n",
    "model_train = model.fit(train_X, train_Y, batch_size=batch_size, epochs=epochs,\n",
    "                        validation_data=(validation_X, validation_Y), shuffle=True)\n",
    "model.save('my_model.h5')\n",
    "history_dict = model_train.history\n",
    "f = open('history.pckl', 'wb')\n",
    "pickle.dump(history_dict, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model was then given unseen data and made to predict vorticity and the results were compared with actual voriticty values. You can check out the results [here](https://1drv.ms/u/s!AmK5uxmgpTOMgct66dv0Xh2ap7QXxA?e=QNSJ48). Each image is made up of three images, the input KE or Temperature, the predicted vorticity from the neural network, and the actual vorticity. As you can see, the neural network does a good job predicting the vorticity, despite only consisting of 6 layers and ~500,000 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use trained model to predict vorticity based on temperature/kinetic energy\n",
    "\n",
    "model = load_model('my_model_KE.h5')    # can change with temperature\n",
    "KE = np.load('kinetic energy test.npy') # can change with temperature\n",
    "vor = np.load('vorticity test.npy')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "n = 30\n",
    "\n",
    "for i in range(n):\n",
    "    KE0 = KE[i*int(5000/n), :]\n",
    "    vor0 = vor[i*int(5000/n), :]\n",
    "    Y = model.predict(KE0.reshape(1, 75, 75, 1))\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    ax1.set_title('Input KE', size=8)\n",
    "    ax1.imshow(KE[i*int(5000/n), :], interpolation='Gaussian')\n",
    "    ax1.axis('off')\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    ax2.set_title('Predicted Output Vorticity', size=8)\n",
    "    ax2.imshow(Y.reshape(75, 75), interpolation='Gaussian')\n",
    "    ax2.axis('off')\n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "    ax3.set_title('Real Vorticity', size=8)\n",
    "    ax3.imshow(vor0, interpolation='Gaussian')\n",
    "    ax3.axis('off')\n",
    "    fig.savefig('image results/' + str(i+1), bbox_inches='tight', dpi=200)\n",
    "    fig.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (science)",
   "language": "python",
   "name": "science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
